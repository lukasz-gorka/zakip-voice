# Backend Communication Logging

Prosty system logowania komunikacji miƒôdzy frontendem a backendem Rust/Tauri.

## Jak w≈ÇƒÖczyƒá logowanie

Ustaw zmiennƒÖ `ASSISTANT_LOG_LEVEL` przed uruchomieniem aplikacji:

```bash
# W≈ÇƒÖcz logowanie (zalecane do debugowania)
ASSISTANT_LOG_LEVEL=info pnpm tauri dev

# Wy≈ÇƒÖcz logowanie (domy≈õlne)
pnpm tauri dev
```

### Inne sposoby:

```bash
# macOS/Linux - eksport na ca≈ÇƒÖ sesjƒô terminala
export ASSISTANT_LOG_LEVEL=info
pnpm tauri dev

# Windows (PowerShell)
$env:ASSISTANT_LOG_LEVEL="info"
pnpm tauri dev

# Windows (CMD)
set ASSISTANT_LOG_LEVEL=info
pnpm tauri dev
```

## Co jest logowane

Gdy logowanie jest w≈ÇƒÖczone, zobaczysz:

### üì• Frontend ‚Üí Backend (Requesty)

Wywo≈Çania komend z frontendu:
```
üì• FRONTEND ‚Üí BACKEND [chat_completion]
{
  "model": "gpt-4",
  "messages": [...]
}
```

### üì§ Backend ‚Üí Frontend (Odpowiedzi)

Odpowiedzi z backendu:
```
üì§ BACKEND ‚Üí FRONTEND [chat_completion]
{
  "response": "...",
  "usage": {...}
}
```

### ‚ùå B≈Çƒôdy

```
‚ùå BACKEND ‚Üí FRONTEND [chat_completion] ERROR: Model not found
```

### üì° Eventy (Streaming)

Eventi wysy≈Çane podczas streamingu:
```
üì° BACKEND EVENT ‚Üí FRONTEND [stream-chunk-session123]
{
  "delta": "Hello"
}

üì° BACKEND EVENT ‚Üí FRONTEND [stream-done-session123]
```

## Przyk≈Çadowy output

```
üîß Backend logging ENABLED (ASSISTANT_LOG_LEVEL=info)

üì• FRONTEND ‚Üí BACKEND [chat_completion_stream]
{
  "request": {
    "model": "gpt-4",
    "messages": [
      {
        "role": "user",
        "content": "Hello!"
      }
    ]
  },
  "session_id": "abc123"
}

üì§ BACKEND ‚Üí FRONTEND [chat_completion_stream]
"Stream started"

üì° BACKEND EVENT ‚Üí FRONTEND [stream-chunk-abc123]
{
  "delta": "Hello"
}

üì° BACKEND EVENT ‚Üí FRONTEND [stream-chunk-abc123]
{
  "delta": " there!"
}

üì° BACKEND EVENT ‚Üí FRONTEND [stream-done-abc123]
```

## Kt√≥re komendy sƒÖ logowane?

Obecnie logowane sƒÖ g≈Ç√≥wne komendy komunikacji z AI:
- `chat_completion` - zwyk≈Çe zapytania do AI
- `chat_completion_stream` - streaming zapytania do AI

Inne komendy (register_provider, set_models, itp.) nie sƒÖ logowane aby nie za≈õmiecaƒá outputu.

## Bezpiecze≈Ñstwo

‚ö†Ô∏è **WA≈ªNE**: Logowanie pokazuje pe≈Çne requesty i odpowiedzi, w≈ÇƒÖcznie z:
- API keys
- Tre≈õƒá wiadomo≈õci
- Dane u≈ºytkownika

**U≈ºywaj logowania tylko w trybie deweloperskim!**

## Wy≈ÇƒÖczanie logowania

Po prostu nie ustawiaj zmiennej `ASSISTANT_LOG_LEVEL`:

```bash
pnpm tauri dev
```

## Implementacja

System logowania to prosty modu≈Ç wykorzystujƒÖcy:
- **src-tauri/src/logger.rs** - Funkcje logowania u≈ºywajƒÖce `println!`
- **log_command! makro** - Automatyczne logowanie request√≥w i odpowiedzi
- **AtomicBool** - Sprawdzanie czy logowanie jest w≈ÇƒÖczone (zero overhead gdy wy≈ÇƒÖczone)

Gdy logowanie jest wy≈ÇƒÖczone, wszystkie funkcje logujƒÖce natychmiast zwracajƒÖ bez ≈ºadnych operacji.

## Rozszerzanie

Aby dodaƒá logowanie do nowej komendy:

```rust
use crate::log_command;

#[tauri::command]
pub async fn my_command(param: String) -> Result<String, String> {
    log_command!("my_command", serde_json::json!({ "param": param }), {
        // Your command logic
        Ok("result".to_string())
    })
}
```

Lub dla bardziej skomplikowanych przypadk√≥w:

```rust
crate::logger::log_command_request("my_command", &request);
// ... your logic ...
crate::logger::log_command_response("my_command", &response);
```
